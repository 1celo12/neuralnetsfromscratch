{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1791244c",
   "metadata": {},
   "source": [
    "# Differential Equation Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ff61ea",
   "metadata": {},
   "source": [
    "### batching \n",
    "\n",
    "* allows for calculations to be done in parallel\n",
    "* distibuted caluclations\n",
    "* scaleable\n",
    "* gives more flexability\n",
    "* Batch training allows for more Efficient training\n",
    "* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f49e83",
   "metadata": {},
   "source": [
    "### common issues\n",
    "\n",
    "* dead network problem where an initial bias is set to zero and is propegated through the network. the model doesnt learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4d1fef",
   "metadata": {},
   "source": [
    "# Abstractions\n",
    "\n",
    "* neuron\n",
    "* layer\n",
    "* activation function\n",
    "* pack propoagation\n",
    "* gradient decent\n",
    "* embedder - *ie onehot embedding for normalizing and scaling\n",
    "* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a59a09",
   "metadata": {},
   "source": [
    "### Optimizers\n",
    "- _Gradient_ : First-order derivative for a multivariate objective function.\n",
    "- _Step Size (alpha)_ : Hyperparameter that controls how far to move in the search space against the gradient each iteration of the algorithm.\n",
    "- algorithms that organize how nerons get adjusted to training data\n",
    "- function that iterates though nerons and changes weights, biases, and activation fucntions to fit data\n",
    "- Stochastic Gradient Decent\n",
    "- Adaptive Movement Estimation (Adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aeff3a",
   "metadata": {},
   "source": [
    "### Activation Functions Layer\n",
    "\n",
    "* ReLu\n",
    "    - efficient\n",
    "    - popular\n",
    "* Step - Hevicide Function\n",
    "    - systen if functions that creates segments with steps (if theta-x > theta-z)\n",
    "* Sigmoid\n",
    "    - slower than relu\n",
    "    - vanishing gradients\n",
    "* Softmax\n",
    "    - used for classification\n",
    "\n",
    "* every neuron in the network has an activation function\n",
    "* connects two nerons\n",
    "\n",
    "* why use it?\n",
    "    - allows for model to segment its self and have each neruon self determine a portion of the data\n",
    "    - with only linear functions, the network can only learn linear data\n",
    "    - using non linear functions allows for fitting of more complex data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75606e48",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "* Categorical Cross Entropy\n",
    " - takes an output from the model, does softmax, compares the normalized output to groundtruth, then finally returns how wrong the values are.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6adcc4",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "* y= e^x\n",
    "* helps encode data with more quality.\n",
    "* OneHotEncoding - encodes data into categorical numbers so that linear neral nets can train on classes.\n",
    "* \"Bias Trick\": adding an extra dimension to the input data (usallly a 1) to implify bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd8c832",
   "metadata": {},
   "source": [
    "### BackPropagation\n",
    "\n",
    "* the process of taking the gradient of the loss with respect to weights, and the gradient of the loss with respect to inputs.\n",
    "\n",
    "* gradient derived from the composite function of all the layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f79446a",
   "metadata": {},
   "source": [
    "\"Backpropagation is really one instance of a more general technique called \"reverse mode differentiation\" to compute derivatives of functions represented in some kind of directed graph form.\" - 3BlueOneBrown (Deep Learning Chapter 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4229af1a",
   "metadata": {},
   "source": [
    "\"Essentially, backpropagation evaluates the expression for the derivative of the cost function as a product of derivatives between each layer from left to right – \"backwards\" – with the gradient of the weights between each layer being a simple modification of the partial products (the \"backwards propagated error).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a03d824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "import math\n",
    "\n",
    "\n",
    "E = math.e\n",
    "\n",
    "# sets default configuration for numpy to avoid data type errors\n",
    "nnfs.init() \n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = self.weights = np.random.randn(n_inputs, n_neurons) * np.sqrt(2 / n_inputs)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    # takes the derivaties of the previous layer\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "    def __repr__(self):\n",
    "        # construct and return a string that represents the network\n",
    "        # architecture\n",
    "        return \"NeuralNetwork: {}\".format(\"-\".join(str(l) for l in self.weights))\n",
    "\n",
    "class ReLu:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        #zero the gradient where inputs are negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "class Sigmoid:\n",
    "    def forward(self, inputs):\n",
    "        self.output = 1/(1 + np.exp(-inputs))\n",
    "\n",
    "class Softmax:\n",
    "    #safe softmax prevents memory overflow\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "class Loss:\n",
    "    def calculate(self, y, output):\n",
    "        sample_losses = self.forward(y, output)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "\n",
    "class CategoricalCrossEntropyLoss(Loss):\n",
    "    def forward(self, y_truth,  y_probs):\n",
    "        samples = len(y_probs)\n",
    "        y_pred_clipped = np.clip(y_probs, 1e-7, 1-1e-7)\n",
    "        \n",
    "        if len(y_truth.shape) == 1: #probabilites for target values, only if categorical labels\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_truth]\n",
    "            \n",
    "        if len(y_truth.shape) == 2: # Masking for one hot encoding\n",
    "            correct_confidences = np.sum(y_pred_clipped*y_truth, axis=1)\n",
    "            \n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def backward(self, y_truth, y_probability):\n",
    "        samples = len(y_probability)\n",
    "        if len(y_truth.shape) == 1:\n",
    "            y_truth = np.eye(y_probability.shape[1])[y_truth]\n",
    "        \n",
    "        self.dinputs = -y_truth/ y_probability\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "class Fast_CCE_Loss: #composition\n",
    "    def __init__(self):\n",
    "        self.activation = Softmax()\n",
    "        self.loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.output = self.activation.output\n",
    "        return self.loss.calculate(y_true, self.output)\n",
    "\n",
    "    def backward(self, dvalues, y_truth):\n",
    "        samples = len(dvalues)\n",
    "        #one-hot encoding conversion\n",
    "        if len(y_truth.shape) == 2:\n",
    "            y_truth = np.argmax(y_truth, axis=1)\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[range(samples), y_truth] -= 1\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "class Gradient_Decent:\n",
    "    def  __init__(self, learning_rate = 0.5):\n",
    "        self.learning_rate= learning_rate\n",
    "        \n",
    "    def update_parameters(self, layer: DenseLayer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases\n",
    "        \n",
    "# TODO: find a way to better format the layers and activations. maybe do it in a way that\n",
    "# accepts a dictionary of objects, check for a design pattern\n",
    "\n",
    "# making a small network\n",
    "class MyFirstNeuralNetwork:\n",
    "    def __init__(self, training, y):\n",
    "        self.data = training\n",
    "        self.truth = y\n",
    "        self.input_layer = DenseLayer(2,64)\n",
    "        self.hidden_layer = DenseLayer(64,3)\n",
    "        self.activation1 = ReLu()\n",
    "        self.activation2 = Fast_CCE_Loss()\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        #layer pass\n",
    "        self.input_layer.forward(self.data)\n",
    "        self.activation1.forward(self.input_layer.output)\n",
    "        \n",
    "        self.hidden_layer.forward(self.activation1.output)\n",
    "        self.loss = self.activation2.forward(self.hidden_layer.output, self.truth)\n",
    "        \n",
    "        #forward pass results\n",
    "        self.probabilities = self.activation2.output\n",
    "        self.predictions = np.argmax(self.probabilities, axis=1)\n",
    "        if len(self.truth.shape) == 2:\n",
    "            self.truth = np.argmax(self.truth, axis=1)\n",
    "        self.accuracy = np.mean(self.predictions == self.truth)\n",
    "        \n",
    "    def backward(self):\n",
    "        self.activation2.backward(self.probabilities, self.truth)\n",
    "        self.hidden_layer.backward(self.activation2.dinputs)\n",
    "        self.activation1.backward(self.hidden_layer.dinputs)\n",
    "        self.input_layer.backward(self.activation1.dinputs)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b810f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nnfs.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt\n",
    "plt.subplots()\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=100, cmap=plt.cm.Spectral)\n",
    "plt.xlabel(\"DATA SET\")\n",
    "plt.show()\n",
    "\n",
    "nn = MyFirstNeuralNetwork(X, y)\n",
    "optimizer = Gradient_Decent()\n",
    "\n",
    "for epoch in range(10000):\n",
    "\n",
    "    nn.forward()\n",
    "    print(f'Epoch:{epoch}, Loss:{nn.loss}, Accuracy:{nn.accuracy}')\n",
    "    nn.backward()\n",
    "    optimizer.update_parameters(nn.hidden_layer)\n",
    "    optimizer.update_parameters(nn.input_layer)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c= nn.predictions, s=100, cmap=plt.cm.Spectral)\n",
    "plt.xlabel(\"PREDICTIONS\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24d7074",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot()\n",
    "plt.plot(nn.activation2.output)\n",
    "plt.xlabel(\"activation layer\")\n",
    "plt.show()\n",
    "plt.scatter(X[:, 0], X[:, 1], c= nn.predictions, s=100, cmap=plt.cm.Spectral)\n",
    "plt.xlabel(\"PREDICTIONS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1452e9d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27552ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = CategoricalCrossEntropyLoss()\n",
    "tensor1 =np.array([[0.1,0.8,0.1],[0.1,0.5,0.4],[0.8,0.1,0.1]])\n",
    "truth = np.array([1,2,1])\n",
    "print(loss.calculate(tensor1, truth)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c4819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da2d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470067db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Church Success \"Churn\" - probability of a church meeting curtain criteria given n features.\n",
    "\n",
    "# total_members, \n",
    "# anual_income, member_frequency(mean%vistors/total_members), social_programs, member_engament, social_media_efficiency_score(mean%retention/platforms), is_membership_increasing, members_youth, members_adults, members_senior, members_family\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d258bca7",
   "metadata": {},
   "source": [
    "### Gradient Decent\n",
    "\n",
    "* consider a chain of nerons\n",
    "    - how can you find the rate each neron changes given an expected output\n",
    "\n",
    "* Chain rule: by taking the derivative of composite functions - ie f'(g'(x)) we can understand how each funciton influences the next.\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a4f07e",
   "metadata": {},
   "source": [
    "## example of composition in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c228aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Component:\n",
    "\n",
    "#    # composite class constructor\n",
    "#     def __init__(self):\n",
    "#         print('Component class object created...')\n",
    "\n",
    "#     # composite class instance method\n",
    "#     def m1(self):\n",
    "#         print('Component class m1() method executed...')\n",
    "\n",
    "\n",
    "class Composite:\n",
    "\n",
    "    # composite class constructor\n",
    "    def __init__(self):\n",
    "\n",
    "        # creating object of component class\n",
    "        self.obj1 = Component()\n",
    "        \n",
    "        print('Composite class object also created...')\n",
    "\n",
    "     # composite class instance method\n",
    "    def m2(self):\n",
    "      \n",
    "        print('Composite class m2() method executed...')\n",
    "    \n",
    "        # calling m1() method of component class\n",
    "        self.obj1.m1()\n",
    "\n",
    "\n",
    "# creating object of composite class\n",
    "obj2 = Composite()\n",
    "\n",
    "# calling m2() method of composite class\n",
    "obj2.m2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47276ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import nnfs\n",
    "# import math\n",
    "\n",
    "\n",
    "# E = math.e\n",
    "\n",
    "# # sets default configuration for numpy to avoid data type errors\n",
    "# nnfs.init() \n",
    "\n",
    "# np.random.seed(42)\n",
    "\n",
    "# class DenseLayer:\n",
    "#     def __init__(self, n_inputs, n_neurons):\n",
    "#         self.weights = self.weights = np.random.randn(n_inputs, n_neurons) * np.sqrt(2 / n_inputs)\n",
    "#         self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         self.inputs = inputs\n",
    "#         self.output = np.dot(inputs, self.weights) + self.biases\n",
    "#     # takes the derivaties of the previous layer\n",
    "    \n",
    "#     def backward(self, dvalues):\n",
    "#         self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "#         self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "#         self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "#         print(self.dweights)\n",
    "#         # print(self.dbiases)\n",
    "#     def __repr__(self):\n",
    "#         # construct and return a string that represents the network\n",
    "#         # architecture\n",
    "#         return \"NeuralNetwork: {}\".format(\"-\".join(str(l) for l in self.weights))\n",
    "\n",
    "# class ReLu:\n",
    "#     def forward(self, inputs):\n",
    "#         self.inputs = inputs\n",
    "#         self.output = np.maximum(0, inputs)\n",
    "    \n",
    "#     def backward(self, dvalues):\n",
    "#         self.dinputs = dvalues.copy()\n",
    "        \n",
    "#         #zero the gradient where inputs are negative\n",
    "#         self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "# class Sigmoid:\n",
    "#     def forward(self, inputs):\n",
    "#         self.output = 1/(1 + np.exp(-inputs))\n",
    "\n",
    "# class Softmax:\n",
    "#     #safe softmax prevents memory overflow\n",
    "#     def forward(self, inputs):\n",
    "#         exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "#         probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "#         self.output = probabilities\n",
    "\n",
    "# class Loss:\n",
    "#     def calculate(self, y, output):\n",
    "#         sample_losses = self.forward(y, output)\n",
    "#         data_loss = np.mean(sample_losses)\n",
    "#         return data_loss\n",
    "\n",
    "# class CategoricalCrossEntropyLoss(Loss):\n",
    "#     def forward(self, y_truth,  y_probs):\n",
    "#         samples = len(y_probs)\n",
    "#         y_pred_clipped = np.clip(y_probs, 1e-7, 1-1e-7)\n",
    "        \n",
    "#         if len(y_truth.shape) == 1: #probabilites for target values, only if categorical labels\n",
    "#             correct_confidences = y_pred_clipped[range(samples), y_truth]\n",
    "            \n",
    "#         if len(y_truth.shape) == 2: # Masking for one hot encoding\n",
    "#             correct_confidences = np.sum(y_pred_clipped*y_truth, axis=1)\n",
    "            \n",
    "#         negative_log_likelihoods = -np.log(correct_confidences)\n",
    "#         return negative_log_likelihoods\n",
    "    \n",
    "#     def backward(self, y_truth, y_probability):\n",
    "#         samples = len(y_probability)\n",
    "#         if len(y_truth.shape) == 1:\n",
    "#             y_truth = np.eye(y_probability.shape[1])[y_truth]\n",
    "        \n",
    "#         self.dinputs = -y_truth/ y_probability\n",
    "#         self.dinputs = self.dinputs / samples\n",
    "\n",
    "# class Fast_CCE_Loss: #composition\n",
    "#     def __init__(self):\n",
    "#         self.activation = Softmax()\n",
    "#         self.loss = CategoricalCrossEntropyLoss()\n",
    "\n",
    "#     def forward(self, inputs, y_true):\n",
    "#         self.activation.forward(inputs)\n",
    "#         self.output = self.activation.output\n",
    "#         return self.loss.calculate(y_true, self.output)\n",
    "\n",
    "#     def backward(self, dvalues, y_truth):\n",
    "#         samples = len(dvalues)\n",
    "#         #one-hot encoding conversion\n",
    "#         if len(y_truth.shape) == 2:\n",
    "#             y_truth = np.argmax(y_truth, axis=1)\n",
    "#         self.dinputs = dvalues.copy()\n",
    "#         self.dinputs[range(samples), y_truth] -= 1\n",
    "#         self.dinputs = self.dinputs / samples\n",
    "\n",
    "# class Gradient_Decent:\n",
    "#     def  __init__(self, learning_rate = 0.01):\n",
    "#         self.learning_rate= learning_rate\n",
    "        \n",
    "#     def update_parameters(self, layer: DenseLayer):\n",
    "#         layer.weights += -self.learning_rate * layer.dweights\n",
    "#         layer.biases += -self.learning_rate * layer.dbiases\n",
    "        \n",
    "# # TODO: find a way to better format the layers and activations. maybe do it in a way that\n",
    "# # accepts a dictionary of objects, check for a design pattern\n",
    "\n",
    "# # making a small network\n",
    "# class MyFirstNeuralNetwork:\n",
    "#     def __init__(self, training, y):\n",
    "#         self.data = training\n",
    "#         self.truth = y # Keep original y as self.truth\n",
    "#         self.input_layer = DenseLayer(2,3)\n",
    "#         self.hidden_layer = DenseLayer(3,3)\n",
    "#         self.activation1 = ReLu()\n",
    "#         self.activation2 = Fast_CCE_Loss()\n",
    "\n",
    "#     def forward(self):\n",
    "#         self.input_layer.forward(self.data)\n",
    "#         self.activation1.forward(self.input_layer.output)\n",
    "\n",
    "#         self.hidden_layer.forward(self.activation1.output)\n",
    "#         # Pass self.truth as is; Fast_CCE_Loss.forward handles 1D/2D conversion for loss calculation\n",
    "#         self.loss = self.activation2.forward(self.hidden_layer.output, self.truth)\n",
    "\n",
    "#         self.probabilities = self.activation2.output\n",
    "#         self.predictions = np.argmax(self.probabilities, axis=1)\n",
    "\n",
    "#         # Store 1D true labels for accuracy and backward pass explicitly\n",
    "#         if len(self.truth.shape) == 2:\n",
    "#             self.truth_labels = np.argmax(self.truth, axis=1)\n",
    "#         else:\n",
    "#             self.truth_labels = self.truth # If original y was already 1D\n",
    "#         self.accuracy = np.mean(self.predictions == self.truth_labels)\n",
    "\n",
    "#     def backward(self):\n",
    "#         # Always pass 1D true labels to Fast_CCE_Loss.backward for consistency\n",
    "#         self.activation2.backward(self.probabilities, self.truth_labels)\n",
    "#         self.hidden_layer.backward(self.activation2.dinputs)\n",
    "#         self.activation1.backward(self.hidden_layer.dinputs)\n",
    "#         self.input_layer.backward(self.activation1.dinputs)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
